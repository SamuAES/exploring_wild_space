{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8801e1e1",
   "metadata": {},
   "source": [
    "## Exploring Wild Space\n",
    "\n",
    "This notebook is the interface to the bird movement feature extraction code.\n",
    "\n",
    "The workflow is divided into two distinct steps:\n",
    "* Step 1: Object detection and raw data extraction. This step applies a YOLO model to video files and saves raw data in JSON format.\n",
    "* Step 2: Feature extraction. Apply the code to extract behavioural features from the JSON files. Results are saved in CSV format.\n",
    "\n",
    "### Step 1: Object detection and raw data extraction\n",
    "\n",
    "In this step, we use the fine-tuned YOLO11n model to detect objects (bird, perches, etc.) in videos. The functionality used here extracts bounding box information for detected objects frame by frame, along with other video metadata (such as the frame rate and dimensions), and saves this data into a JSON file in the specified directory. Each video will have a corresponding JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267dc62",
   "metadata": {},
   "source": [
    "In the configuration cell below, define the path to a video file or a directory contining multiple video files. Define also the model path and the output directory (where results are saved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56628923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.frames import read_video_and_save_frames_to_json\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Option 1: Specify a single video file\n",
    "# input_path = 'data/original_videos/CAGE_250520_HA70342_exploration_IB.mp4' \n",
    " \n",
    "# Option 2: Specify a directory containing video files\n",
    "input_path = 'data/original_videos/' \n",
    "\n",
    "output_dir = 'data/raw_data/'\n",
    "model_path = 'yolo/custom_yolo11n_v2.pt'\n",
    "allowed_extensions = ['.mp4', '.avi', '.mov'] # Add other video extensions if needed\n",
    "# --- End Configuration ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61176261",
   "metadata": {},
   "source": [
    "Run the next cell to gather videos for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Determine video paths to process\n",
    "video_paths_to_process = []\n",
    "if os.path.isfile(input_path):\n",
    "    video_paths_to_process.append(input_path)\n",
    "    print(f\"Processing single video file: {input_path}\")\n",
    "elif os.path.isdir(input_path):\n",
    "    print(f\"Processing all videos in directory: {input_path}\")\n",
    "    for filename in os.listdir(input_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in allowed_extensions):\n",
    "            video_paths_to_process.append(os.path.join(input_path, filename))\n",
    "else:\n",
    "    print(f\"Error: Input path not found or is not a valid file/directory: {input_path}\")\n",
    "\n",
    "print(f\"Found {len(video_paths_to_process)} video(s) to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f282d",
   "metadata": {},
   "source": [
    "Run the next cell to extract data from the videos using the YOLO model specified above. The raw data is saved in JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Object Detection ---\n",
    "if not 'video_paths_to_process' in locals() or not video_paths_to_process:\n",
    "    print(\"No video paths defined or found in the previous cell. Please run the previous cell first.\")\n",
    "else:\n",
    "    total_videos = len(video_paths_to_process) # Get total number of videos\n",
    "    print(f\"Starting object detection for {total_videos} video(s)...\")\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, video_path in enumerate(video_paths_to_process): # Use enumerate for index\n",
    "        video_filename = os.path.basename(video_path)\n",
    "        output_json_filename = os.path.splitext(video_filename)[0] + '.json'\n",
    "        output_json_path = os.path.join(output_dir, output_json_filename)\n",
    "\n",
    "        # Print progress before processing the current video\n",
    "        print(f\"\\nProcessing video {idx + 1} of {total_videos}: {video_filename}...\") \n",
    "        \n",
    "        if os.path.exists(video_path):\n",
    "            try:\n",
    "                read_video_and_save_frames_to_json(\n",
    "                    video_filepath=video_path, \n",
    "                    model_path=model_path, \n",
    "                    save_path=output_json_path\n",
    "                    #max_frames=None # Optional: uncomment to limit frames\n",
    "                )\n",
    "                print(f\"Saved detection data to {output_json_path}\")\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "                error_count += 1\n",
    "        else:\n",
    "            print(f\"Video file not found : {video_path}\")\n",
    "            error_count += 1\n",
    "            \n",
    "    print(f\"\\n--- Detection Summary ---\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Errors encountered: {error_count}\")\n",
    "    print(f\"Total attempted: {total_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761e486",
   "metadata": {},
   "source": [
    "### Step 2: Feature Extraction\n",
    "\n",
    "In this step, we load the JSON files generated in the previous step (which are located in `data/raw_data/` by default). Then, functions from other modules are used to compute the relevant behavioural features. Refer to the tables below for the feature definitions.\n",
    "\n",
    "The extracted features and quality metrics are saved to CSV files in the `data/extracted_features/` directory.\n",
    "\n",
    "#### Extracted Features\n",
    "\n",
    "| Feature     | Unit         | Description                                                                 |\n",
    "|-------------|--------------|-----------------------------------------------------------------------------|\n",
    "| latency     | Duration (s) | Time until first entry into the novel (exploration) area.                   |\n",
    "| 5perches    | Duration (s) | Time spent in the novel area until the 5th distinct perch (1-5) is visited. |\n",
    "| ground      | Duration (s) | Total time spent on the ground.                                             |\n",
    "| perch1      | Duration (s) | Total time spent on perch 1.                                                |\n",
    "| perch2      | Duration (s) | Total time spent on perch 2.                                                |\n",
    "| perch3      | Duration (s) | Total time spent on perch 3.                                                |\n",
    "| perch4      | Duration (s) | Total time spent on perch 4.                                                |\n",
    "| perch5      | Duration (s) | Total time spent on perch 5.                                                |\n",
    "| movements   | Count        | Number of movements (hops/flights) detected in the novel area.              |\n",
    "| back_home   | Duration (s) | Time until the bird first returns to the home area after entering novel area. |\n",
    "| T_new       | Duration (s) | Total time spent in the novel (exploration) area.                           |\n",
    "| T_home      | Duration (s) | Total time spent in the home area.                                          |\n",
    "| move_home   | Count        | Number of movements (hops/flights) detected in the home area.               |\n",
    "| top         | Duration (s) | Total time spent in the top section of the cage.                            |\n",
    "| middle      | Duration (s) | Total time spent in the middle section of the cage.                         |\n",
    "| bottom      | Duration (s) | Total time spent in the bottom section of the cage.                         |\n",
    "| fence       | Duration (s) | Total time spent detected near the fence/mesh.                              |\n",
    "\n",
    "#### Quality Metrics\n",
    "\n",
    "| Metric                   | Unit         | Description                                                                                 |\n",
    "|--------------------------|--------------|---------------------------------------------------------------------------------------------|\n",
    "| camera_movement        | Boolean      | Indicates if significant camera/perch coordinate movement was detected during analysis.     |\n",
    "| perch_count            | Count        | Number of perches (out of 5) reliably identified in the novel area in initial frames.     |\n",
    "| close_perches          | Boolean      | Indicates if any identified perches (1-5) are potentially too close together.               |\n",
    "| bird_inbetween_zones   | Rate (ev/s)  | Rate at which the bird was detected in ambiguous vertical zone boundaries (events per sec). |\n",
    "| bird_inbetween_perches | Rate (ev/s)  | Rate at which the bird was detected in ambiguous location between perches 2 & 3 (events per sec). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dba0ef",
   "metadata": {},
   "source": [
    "In the configuration below, define the path to JSON file or a directory containing multiple JSON files. Define also the results directory (where results are saved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Option 1: Specify a single JSON file\n",
    "#json_input_path = 'data/raw_data/CAGE_200520_HA70336_exploration_IB.json'\n",
    "\n",
    "# Option 2: Specify a directory containing JSON files\n",
    "json_input_path = 'data/raw_data/'\n",
    "\n",
    "# Output directory for extracted features\n",
    "output_features_dir = 'data/extracted_features/'\n",
    "allowed_json_extensions = ['.json']\n",
    "\n",
    "# Feature Extraction Parameters\n",
    "# Note: hese params have been set to sensible defaults and\n",
    "# it is not necessary to change them. Both must be odd.\n",
    "window_size_mean = 5 \n",
    "window_size_mode = 35\n",
    "# --- End Configuration ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75658241",
   "metadata": {},
   "source": [
    "Run the cell below to gather the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d03e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(output_features_dir, exist_ok=True)\n",
    "\n",
    "# Determine JSON paths to process\n",
    "json_paths_to_process = []\n",
    "if os.path.isfile(json_input_path):\n",
    "    if any(json_input_path.lower().endswith(ext) for ext in allowed_json_extensions):\n",
    "        json_paths_to_process.append(json_input_path)\n",
    "        print(f\"Processing single JSON file: {json_input_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Specified file is not a JSON file: {json_input_path}\")\n",
    "elif os.path.isdir(json_input_path):\n",
    "    print(f\"Processing all JSON files in directory: {json_input_path}\")\n",
    "    for filename in os.listdir(json_input_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in allowed_json_extensions):\n",
    "            json_paths_to_process.append(os.path.join(json_input_path, filename))\n",
    "else:\n",
    "    print(f\"Error: Input path not found or is not a valid file/directory: {json_input_path}\")\n",
    "\n",
    "print(f\"Found {len(json_paths_to_process)} JSON file(s) to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff41b",
   "metadata": {},
   "source": [
    "Run the cell below to apply feature extraction to the JSON files. The results are saved in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb715faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.frames import load_json_to_dict\n",
    "from utils.features import extract_features, save_features_to_csv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Feature Extraction Loop ---\n",
    "if not 'json_paths_to_process' in locals() or not json_paths_to_process:\n",
    "    print(\"No JSON paths defined or found. Please run the previous cell first.\")\n",
    "else:\n",
    "    total_files = len(json_paths_to_process)\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    error_files = [] # Initialize list to store filenames with errors\n",
    "    all_features_list = [] # Optional: Collect all features in a list\n",
    "    all_quality_list = []  # Optional: Collect all quality metrics\n",
    "    \n",
    "    # Use a list to store iteration messages (status + result)\n",
    "    # Each item will be a tuple: (status_line, result_line)\n",
    "    last_outputs = []\n",
    "\n",
    "    for idx, json_path in enumerate(json_paths_to_process):\n",
    "        clear_output(wait=True)\n",
    "        for output in last_outputs:\n",
    "            print(output)\n",
    "\n",
    "        json_filename = os.path.basename(json_path)\n",
    "        base_filename = os.path.splitext(json_filename)[0]\n",
    "        print(f\"\\nProcessing file {idx + 1} of {total_files}: {json_filename}...\")\n",
    "\n",
    "        try:\n",
    "            # 1. Load JSON data\n",
    "            raw_data = load_json_to_dict(json_path)\n",
    "\n",
    "            # Extract necessary parameters from loaded data\n",
    "            fps = raw_data.get('fps')\n",
    "            frame_count = raw_data.get('frame_count')\n",
    "            frames_data = raw_data.get('frames') # Check if frames data exists\n",
    "\n",
    "            if fps is None or frame_count is None or frames_data is None:\n",
    "                error_msg = f\"Missing required keys (fps, frame_count, frames) in JSON file.\"\n",
    "                last_outputs.append(f\"Error processing {json_filename}: {error_msg}\")\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "            # 2. Extract Features\n",
    "            features_df, bird_status_array, quality_df = extract_features(\n",
    "                data_raw=raw_data, # Pass the whole loaded dict\n",
    "                window_size_mean=window_size_mean,\n",
    "                window_size_mode=window_size_mode,\n",
    "                fps=int(fps),\n",
    "                frame_count=int(frame_count)\n",
    "            )\n",
    "            print(\"Features extracted successfully.\")\n",
    "\n",
    "            # Optional: Add identifier and collect DataFrames\n",
    "            features_df['identifier'] = base_filename\n",
    "            quality_df['identifier'] = base_filename\n",
    "            all_features_list.append(features_df)\n",
    "            all_quality_list.append(quality_df)\n",
    "\n",
    "            # 3. Save Features to CSV\n",
    "            print(\"Saving features to CSV...\")\n",
    "            save_features_to_csv(\n",
    "                features_df=features_df,\n",
    "                bird_status=bird_status_array,\n",
    "                quality_df=quality_df,\n",
    "                base_filename=base_filename,\n",
    "                output_dir=output_features_dir\n",
    "            )\n",
    "            processed_count += 1\n",
    "            last_outputs.append(f\"Processed {json_filename} successfully.\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_filename}: {e}\")\n",
    "            error_count += 1\n",
    "            error_files.append((json_filename,e)) # Add filename to error list\n",
    "            last_outputs.append(f\"Error processing {json_filename}: {e}\")\n",
    "\n",
    "    clear_output(wait=False)\n",
    "    print(f\"\\n--- Feature Extraction Summary ---\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Errors encountered: {error_count}\")\n",
    "    print(f\"Total attempted: {total_files}\")\n",
    "\n",
    "    # Print filenames that caused errors\n",
    "    if error_files:\n",
    "        print(\"\\n--- Files with Errors ---\")\n",
    "        for filename, error in error_files:\n",
    "            print(f\"{filename}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8441baf",
   "metadata": {},
   "source": [
    "Finally, run the last cell to combine results and save them into separate CSV file. This step is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077020b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Combine all features into single DataFrames if needed\n",
    "if all_features_list:\n",
    "    combined_features_df = pd.concat(all_features_list, ignore_index=True)\n",
    "    combined_quality_df = pd.concat(all_quality_list, ignore_index=True)\n",
    "    print(\"\\nCombined features DataFrame head:\")\n",
    "    # display(combined_features_df.head()) # Use display in Jupyter\n",
    "    print(combined_features_df.head())\n",
    "    print(\"\\nCombined quality metrics DataFrame head:\")\n",
    "    # display(combined_quality_df.head())\n",
    "    print(combined_quality_df.head())\n",
    "    \n",
    "    # You could save these combined dataframes as well if desired\n",
    "    combined_features_df.to_csv(os.path.join(output_features_dir, 'all_features_combined4.csv'), index=False)\n",
    "    combined_quality_df.to_csv(os.path.join(output_features_dir, 'all_quality_combined4.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
