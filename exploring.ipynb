{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8801e1e1",
   "metadata": {},
   "source": [
    "# Step 1: Object Detection and Data Extraction\n",
    "\n",
    "Use the custom YOLOv11nano model to detect objects (bird, perches, etc.) in the videos located in `data/original_videos/`.\n",
    "The function `read_video_and_save_frames_to_json` from `utils/frames.py` processes each video.\n",
    "It extracts bounding box information for detected objects frame by frame, along with other video metadata (like frame rate and dimensions), and saves this data into a JSON file in the `data/raw_data/` directory.\n",
    "Each video will have a corresponding JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267dc62",
   "metadata": {},
   "source": [
    "Define path to video file or a directory that contains video files to be processed. Define path where to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188e0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all videos in directory: data/original_videos/\n",
      "Found 11 video(s) to process.\n"
     ]
    }
   ],
   "source": [
    "from utils.frames import read_video_and_save_frames_to_json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Option 1: Specify a single video file\n",
    "# input_path = 'data/original_videos/CAGE_250520_HA70342_exploration_IB.mp4' \n",
    " \n",
    "# Option 2: Specify a directory containing video files\n",
    "input_path = 'data/original_videos/' \n",
    "\n",
    "output_dir = 'data/raw_data/'\n",
    "model_path = 'yolo/custom_yolo11n_v2.pt'\n",
    "allowed_extensions = ['.mp4', '.avi', '.mov'] # Add other video extensions if needed\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Determine video paths to process\n",
    "video_paths_to_process = []\n",
    "if os.path.isfile(input_path):\n",
    "    video_paths_to_process.append(input_path)\n",
    "    print(f\"Processing single video file: {input_path}\")\n",
    "elif os.path.isdir(input_path):\n",
    "    print(f\"Processing all videos in directory: {input_path}\")\n",
    "    for filename in os.listdir(input_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in allowed_extensions):\n",
    "            video_paths_to_process.append(os.path.join(input_path, filename))\n",
    "else:\n",
    "    print(f\"Error: Input path not found or is not a valid file/directory: {input_path}\")\n",
    "\n",
    "print(f\"Found {len(video_paths_to_process)} video(s) to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f282d",
   "metadata": {},
   "source": [
    "Use custom YOLO model to detect objects from video files and store information of bounding boxes found in each frame into JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09dbad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object detection for 11 video(s)...\n",
      "HE21360_100721_21OW8_exploration_IB.json\n",
      "data/raw_data/HE21360_100721_21OW8_exploration_IB.json\n",
      "\n",
      "Processing video 1 of 11: HE21360_100721_21OW8_exploration_IB.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frames:   0%|          | 17/18024 [00:04<1:21:43,  3.67it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(video_path):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Corrected parameter name from output_json_path to save_path\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Removed conf and classes parameters as they are not accepted by the function\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         \u001b[43mread_video_and_save_frames_to_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvideo_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_json_path\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# max_frames=None # Optional: uncomment to limit frames\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved detection data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m         processed_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/utils/frames.py:143\u001b[0m, in \u001b[0;36mread_video_and_save_frames_to_json\u001b[0;34m(video_filepath, save_path, model_path, max_frames)\u001b[0m\n\u001b[1;32m    132\u001b[0m fps \u001b[38;5;241m=\u001b[39m  video_capture\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS)\n\u001b[1;32m    134\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_filename\u001b[39m\u001b[38;5;124m\"\u001b[39m:filename,\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_count\u001b[39m\u001b[38;5;124m\"\u001b[39m:frame_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}\n\u001b[1;32m    141\u001b[0m     }\n\u001b[0;32m--> 143\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFrames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmininterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/utils/frames.py:65\u001b[0m, in \u001b[0;36mread_video\u001b[0;34m(video_capture, model_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     63\u001b[0m results \u001b[38;5;241m=\u001b[39m model(frame, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Run YOLO detection on frame\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# class names\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/engine/predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:558\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 558\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:110\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:149\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    150\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:1042\u001b[0m, in \u001b[0;36mC2PSA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m a, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat((a, b), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:956\u001b[0m, in \u001b[0;36mPSABlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    955\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes a forward pass through PSABlock, applying attention and feed-forward layers to the input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m    957\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Github/exploring_wild_space/.venv/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:918\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    913\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, N)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    914\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    915\u001b[0m )\n\u001b[1;32m    917\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m k) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 918\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m x \u001b[38;5;241m=\u001b[39m (v \u001b[38;5;241m@\u001b[39m attn\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(B, C, H, W) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(v\u001b[38;5;241m.\u001b[39mreshape(B, C, H, W))\n\u001b[1;32m    920\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Object Detection ---\n",
    "if not 'video_paths_to_process' in locals() or not video_paths_to_process:\n",
    "    print(\"No video paths defined or found in the previous cell. Please run the previous cell first.\")\n",
    "else:\n",
    "    total_videos = len(video_paths_to_process) # Get total number of videos\n",
    "    print(f\"Starting object detection for {total_videos} video(s)...\")\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, video_path in enumerate(video_paths_to_process): # Use enumerate for index\n",
    "        video_filename = os.path.basename(video_path)\n",
    "        output_json_filename = os.path.splitext(video_filename)[0] + '.json'\n",
    "        output_json_path = os.path.join(output_dir, output_json_filename)\n",
    "\n",
    "        # Print progress before processing the current video\n",
    "        print(f\"\\nProcessing video {idx + 1} of {total_videos}: {video_filename}...\") \n",
    "        \n",
    "        if os.path.exists(video_path):\n",
    "            try:\n",
    "                read_video_and_save_frames_to_json(\n",
    "                    video_filepath=video_path, \n",
    "                    model_path=model_path, \n",
    "                    save_path=output_json_path\n",
    "                    # max_frames=None # Optional: uncomment to limit frames\n",
    "                )\n",
    "                print(f\"Saved detection data to {output_json_path}\")\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "                error_count += 1\n",
    "        else:\n",
    "            print(f\"Video file not found : {video_path}\")\n",
    "            error_count += 1\n",
    "            \n",
    "    print(f\"\\n--- Detection Summary ---\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Errors encountered: {error_count}\")\n",
    "    print(f\"Total attempted: {total_videos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761e486",
   "metadata": {},
   "source": [
    "# Step 2: Feature Extraction\n",
    "\n",
    "Load the JSON files generated in the previous step (located in `data/raw_data/`).\n",
    "Use the `extract_features` function from `utils/features.py` to calculate relevant behavioral features from the raw detection data.\n",
    "The extracted features and quality metrics are saved to separate CSV files in the `data/extracted_features/` directory using the `save_features_to_csv` function.\n",
    "\n",
    "## Extracted Features\n",
    "\n",
    "| Feature     | Unit         | Description                                                                 |\n",
    "|-------------|--------------|-----------------------------------------------------------------------------|\n",
    "| latency     | Duration (s) | Time until first entry into the novel (exploration) area.                   |\n",
    "| 5perches    | Duration (s) | Time spent in the novel area until the 5th distinct perch (1-5) is visited. |\n",
    "| ground      | Duration (s) | Total time spent on the ground.                                             |\n",
    "| perch1      | Duration (s) | Total time spent on perch 1.                                                |\n",
    "| perch2      | Duration (s) | Total time spent on perch 2.                                                |\n",
    "| perch3      | Duration (s) | Total time spent on perch 3.                                                |\n",
    "| perch4      | Duration (s) | Total time spent on perch 4.                                                |\n",
    "| perch5      | Duration (s) | Total time spent on perch 5.                                                |\n",
    "| movements   | Count        | Number of movements (hops/flights) detected in the novel area.              |\n",
    "| back_home   | Duration (s) | Time until the bird first returns to the home area after entering novel area. |\n",
    "| T_new       | Duration (s) | Total time spent in the novel (exploration) area.                           |\n",
    "| T_home      | Duration (s) | Total time spent in the home area.                                          |\n",
    "| move_home   | Count        | Number of movements (hops/flights) detected in the home area.               |\n",
    "| top         | Duration (s) | Total time spent in the top section of the cage.                            |\n",
    "| middle      | Duration (s) | Total time spent in the middle section of the cage.                         |\n",
    "| bottom      | Duration (s) | Total time spent in the bottom section of the cage.                         |\n",
    "| fence       | Duration (s) | Total time spent detected near the fence/mesh.                              |\n",
    "\n",
    "## Quality Metrics\n",
    "\n",
    "| Metric                   | Unit         | Description                                                                                 |\n",
    "|--------------------------|--------------|---------------------------------------------------------------------------------------------|\n",
    "| camera_movement        | Boolean      | Indicates if significant camera/perch coordinate movement was detected during analysis.     |\n",
    "| perch_count            | Count        | Number of perches (out of 5) reliably identified in the novel area in initial frames.     |\n",
    "| close_perches          | Boolean      | Indicates if any identified perches (1-5) are potentially too close together.               |\n",
    "| bird_inbetween_zones   | Rate (ev/s)  | Rate at which the bird was detected in ambiguous vertical zone boundaries (events per sec). |\n",
    "| bird_inbetween_perches | Rate (ev/s)  | Rate at which the bird was detected in ambiguous location between perches 2 & 3 (ev per sec). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dba0ef",
   "metadata": {},
   "source": [
    "Define path to JSON file or a directory that contains JSON files to be processed. Define path where to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d03e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all JSON files in directory: data/raw_data/\n",
      "Found 21 JSON file(s) to process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Option 1: Specify a single JSON file\n",
    "#json_input_path = 'data/raw_data/CAGE_200520_HA70336_exploration_IB.json'\n",
    "\n",
    "# Option 2: Specify a directory containing JSON files\n",
    "json_input_path = 'data/raw_data/'\n",
    "\n",
    "# Output directory for extracted features\n",
    "output_features_dir = 'data/extracted_features/'\n",
    "\n",
    "allowed_json_extensions = ['.json']\n",
    "\n",
    "# Feature Extraction Parameters\n",
    "window_size_mean = 5  # Adjust as needed. Must be odd.\n",
    "window_size_mode = 31  # Adjust as needed. Must be odd.\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_features_dir, exist_ok=True)\n",
    "\n",
    "# Determine JSON paths to process\n",
    "json_paths_to_process = []\n",
    "if os.path.isfile(json_input_path):\n",
    "    if any(json_input_path.lower().endswith(ext) for ext in allowed_json_extensions):\n",
    "        json_paths_to_process.append(json_input_path)\n",
    "        print(f\"Processing single JSON file: {json_input_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Specified file is not a JSON file: {json_input_path}\")\n",
    "elif os.path.isdir(json_input_path):\n",
    "    print(f\"Processing all JSON files in directory: {json_input_path}\")\n",
    "    for filename in os.listdir(json_input_path):\n",
    "        if any(filename.lower().endswith(ext) for ext in allowed_json_extensions):\n",
    "            json_paths_to_process.append(os.path.join(json_input_path, filename))\n",
    "else:\n",
    "    print(f\"Error: Input path not found or is not a valid file/directory: {json_input_path}\")\n",
    "\n",
    "print(f\"Found {len(json_paths_to_process)} JSON file(s) to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aff41b",
   "metadata": {},
   "source": [
    "Extract features from JSON files and store results into CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb715faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Extraction Summary ---\n",
      "Successfully processed: 20\n",
      "Errors encountered: 1\n",
      "Total attempted: 21\n",
      "\n",
      "--- Files with Errors ---\n",
      "CAGE_220520_HA70337_exploration_IB.json: No perches found in the frame.\n"
     ]
    }
   ],
   "source": [
    "from utils.frames import load_json_to_dict\n",
    "from utils.features import extract_features, save_features_to_csv\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- Feature Extraction Loop ---\n",
    "if not 'json_paths_to_process' in locals() or not json_paths_to_process:\n",
    "    print(\"No JSON paths defined or found. Please run the previous cell first.\")\n",
    "else:\n",
    "    total_files = len(json_paths_to_process)\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    error_files = [] # Initialize list to store filenames with errors\n",
    "    all_features_list = [] # Optional: Collect all features in a list\n",
    "    all_quality_list = []  # Optional: Collect all quality metrics\n",
    "    \n",
    "    # Use a list to store iteration messages (status + result)\n",
    "    # Each item will be a tuple: (status_line, result_line)\n",
    "    last_outputs = []\n",
    "\n",
    "    for idx, json_path in enumerate(json_paths_to_process):\n",
    "        clear_output(wait=True)\n",
    "        for output in last_outputs:\n",
    "            print(output)\n",
    "\n",
    "        json_filename = os.path.basename(json_path)\n",
    "        base_filename = os.path.splitext(json_filename)[0]\n",
    "        print(f\"\\nProcessing file {idx + 1} of {total_files}: {json_filename}...\")\n",
    "\n",
    "        try:\n",
    "            # 1. Load JSON data\n",
    "            raw_data = load_json_to_dict(json_path)\n",
    "\n",
    "            # Extract necessary parameters from loaded data\n",
    "            fps = raw_data.get('fps')\n",
    "            frame_count = raw_data.get('frame_count')\n",
    "            frames_data = raw_data.get('frames') # Check if frames data exists\n",
    "\n",
    "            if fps is None or frame_count is None or frames_data is None:\n",
    "                error_msg = f\"Missing required keys (fps, frame_count, frames) in JSON file.\"\n",
    "                last_outputs.append(f\"Error processing {json_filename}: {error_msg}\")\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "            # 2. Extract Features\n",
    "            features_df, bird_status_array, quality_df = extract_features(\n",
    "                data_raw=raw_data, # Pass the whole loaded dict\n",
    "                window_size_mean=window_size_mean,\n",
    "                window_size_mode=window_size_mode,\n",
    "                fps=int(fps),\n",
    "                frame_count=int(frame_count)\n",
    "            )\n",
    "            print(\"Features extracted successfully.\")\n",
    "\n",
    "            # Optional: Add identifier and collect DataFrames\n",
    "            features_df['identifier'] = base_filename\n",
    "            quality_df['identifier'] = base_filename\n",
    "            all_features_list.append(features_df)\n",
    "            all_quality_list.append(quality_df)\n",
    "\n",
    "            # 3. Save Features to CSV\n",
    "            print(\"Saving features to CSV...\")\n",
    "            save_features_to_csv(\n",
    "                features_df=features_df,\n",
    "                bird_status=bird_status_array,\n",
    "                quality_df=quality_df,\n",
    "                base_filename=base_filename,\n",
    "                output_dir=output_features_dir\n",
    "            )\n",
    "            processed_count += 1\n",
    "            last_outputs.append(f\"Processed {json_filename} successfully.\")\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_filename}: {e}\")\n",
    "            error_count += 1\n",
    "            error_files.append((json_filename,e)) # Add filename to error list\n",
    "            last_outputs.append(f\"Error processing {json_filename}: {e}\")\n",
    "\n",
    "    clear_output(wait=False)\n",
    "    print(f\"\\n--- Feature Extraction Summary ---\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Errors encountered: {error_count}\")\n",
    "    print(f\"Total attempted: {total_files}\")\n",
    "\n",
    "    # Print filenames that caused errors\n",
    "    if error_files:\n",
    "        print(\"\\n--- Files with Errors ---\")\n",
    "        for filename, error in error_files:\n",
    "            print(f\"{filename}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8441baf",
   "metadata": {},
   "source": [
    "Combine results and save them into separate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077020b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined features DataFrame head:\n",
      "       perch1     perch2      perch3     perch4  perch5    5perches  \\\n",
      "0  116.133333  78.166667  109.900000  37.700000    54.4   10.066667   \n",
      "1    0.000000   0.000000   33.600000  75.933333    36.3  147.433333   \n",
      "2    0.000000   0.000000    0.000000   0.000000     0.0    0.000000   \n",
      "3  170.566667  61.400000  138.066667   0.700000     4.5    8.333333   \n",
      "4    0.000000   0.000000    0.000000   0.000000     0.0    0.000000   \n",
      "\n",
      "        fence    ground     top  middle  bottom   T_new  T_home  latency  \\\n",
      "0   17.200000  0.000000  267.60  326.77    5.93  415.60  184.70     17.8   \n",
      "1  329.666667  0.000000  170.07  429.87    0.00  147.43  452.50    413.7   \n",
      "2    0.866667  0.000000   85.10   62.30  453.33    0.00  600.73      NaN   \n",
      "3  135.300000  1.633333  255.73  336.83    8.07  538.67   61.97     62.0   \n",
      "4    0.000000  0.000000  599.70    0.00    0.00    0.00  599.70      NaN   \n",
      "\n",
      "    back_home  move_home  movements  home_perches_identified  \\\n",
      "0   58.133333         57         91                        1   \n",
      "1  529.500000          3          9                        0   \n",
      "2         NaN          2          0                        0   \n",
      "3         NaN         11        306                        1   \n",
      "4         NaN          0          0                        1   \n",
      "\n",
      "                             identifier  \n",
      "0  HE21364_110721_21NB25_exploration_IB  \n",
      "1    CAGE_020720_HA70343_exploration_IB  \n",
      "2    CAGE_200520_HA70335_exploration_IB  \n",
      "3  HE21365_110721_21NB23_exploration_IB  \n",
      "4    CAGE_030720_HA70344_exploration_IB  \n",
      "\n",
      "Combined quality metrics DataFrame head:\n",
      "   camera_movement  perch_count  close_perches  bird_inbetween_zones  \\\n",
      "0            False            5          False            160.466667   \n",
      "1            False            5           True             95.466667   \n",
      "2            False            5          False             52.666667   \n",
      "3             True            5          False            159.400000   \n",
      "4            False            5           True              0.000000   \n",
      "\n",
      "   bird_inbetween_perches                            identifier  \n",
      "0               81.766667  HE21364_110721_21NB25_exploration_IB  \n",
      "1               11.300000    CAGE_020720_HA70343_exploration_IB  \n",
      "2                0.000000    CAGE_200520_HA70335_exploration_IB  \n",
      "3              122.133333  HE21365_110721_21NB23_exploration_IB  \n",
      "4                0.000000    CAGE_030720_HA70344_exploration_IB  \n"
     ]
    }
   ],
   "source": [
    "# Optional: Combine all features into single DataFrames if needed\n",
    "if all_features_list:\n",
    "    combined_features_df = pd.concat(all_features_list, ignore_index=True)\n",
    "    combined_quality_df = pd.concat(all_quality_list, ignore_index=True)\n",
    "    print(\"\\nCombined features DataFrame head:\")\n",
    "    # display(combined_features_df.head()) # Use display in Jupyter\n",
    "    print(combined_features_df.head())\n",
    "    print(\"\\nCombined quality metrics DataFrame head:\")\n",
    "    # display(combined_quality_df.head())\n",
    "    print(combined_quality_df.head())\n",
    "    \n",
    "    # You could save these combined dataframes as well if desired\n",
    "    combined_features_df.to_csv(os.path.join(output_features_dir, 'all_features_combined4.csv'), index=False)\n",
    "    combined_quality_df.to_csv(os.path.join(output_features_dir, 'all_quality_combined4.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
